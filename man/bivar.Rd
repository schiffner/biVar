% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bivar.R
\name{bivar}
\alias{bivar}
\alias{bivar.data.frame}
\alias{bivar.default}
\title{Bias-Variance Decomposition of the Misclassification Rate}
\usage{
bivar(y, ...)

\method{bivar}{data.frame}(y, ...)

\method{bivar}{default}(y, grouping, ybayes, posterior, ybest = NULL, ...)
}
\arguments{
\item{y}{Predicted class labels on a test data set based on multiple training data sets. 
For the default method \code{y} is supposed to be a \code{list} where each element contains 
the predictions for one single test observation. The list elements are supposed to be \code{factor}s
with the same levels as \code{grouping}.
\code{y} can also be a \code{data.frame} where the rows correspond to test observations and the columns
correspond to predictions on these test observations based on the different training sets.}

\item{grouping}{Vector of true class labels (a \code{factor}).}

\item{ybayes}{(Optional.) Bayes prediction (a \code{factor} with the same levels as \code{grouping}). 
Ignored if \code{posterior} is specified as \code{ybayes} can be easily calculated from the posterior
probabilities.}

\item{posterior}{(Optional.) Matrix of posterior probabilities, either known or estimated. It is assumed
that the columns are ordered according to the factor levels of \code{grouping}.}

\item{ybest}{Prediction from the best fitting model on the whole population (a \code{factor} with the
same levels as \code{grouping}). 
Used for calculation of model and estimation bias as well as systematic model effect and systematic
estimation effect.}

\item{\dots}{Currently unused.}
}
\value{
A \code{data.frame} with the following columns:
\item{error}{Estimated misclassification probability.}
\item{noise}{(Only if \code{ybayes} or \code{posterior} was specified.) Noise or Bayes error rate.}
\item{bias}{Bias.}
\item{model.bias}{(Only if \code{ybest} was specified.) Model bias.}
\item{estimation.bias}{(Only if \code{ybest} was specified.) Estimation bias.}
\item{variance}{Variance.}
\item{unbiased.variance}{Unbiased variance.}
\item{biased.variance}{Biased variance.}
\item{net.variance}{Point-wise net variance.}
\item{systematic.effect}{Systematic effect.}
\item{systematic.model.effect}{(Only if \code{ybest} was specified.) Systematic model effect.}
\item{systematic.estimation.effect}{(Only if \code{ybest} was specified.) Systematic estimation effect.}
\item{variance.effect}{Variance effect.}
\item{ymain}{Main prediction.}
\item{ybayes}{(Only if \code{ybayes} or \code{posterior} was specified.) The optimal prediction.}
\item{size}{Numeric vector of the same length as the number of test observations. The number of predictions made for each test observation.}
}
\description{
Computes the bias-variance decomposition of the misclassification rate according to the approaches
of James (2003) and Domingos (2000).
}
\details{
If \code{posterior} is specified, \code{ybayes} is calculated from the posterior probabilities and
the posteriors are used to  calculate/estimate noise, the misclassification rate, systematic effect
and variance effect.
If \code{ybayes} is specified it is ignored if \code{posterior} is given. Otherwise the empirical
distribution of \code{ybayes} is inferred and used to calculate the quantities of interest.
If neither \code{posterior} nor \code{ybayes} are specified it is assumed that the noise level is
zero and the remaining quantities are calculated based on this supposition.
}
\references{
Domingos, P. (2000). A unified bias-variance decomposition for zero-one and squared loss. In
Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on 
Innovative Applications of Artificial Intelligence, pages 564--569. AAAI Press / The MIT Press.

James, G. M. (2003). Variance and bias for general loss functions. \emph{Machine Learning}, \bold{51(2)} 115--135.
}

